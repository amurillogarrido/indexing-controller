import streamlit as st
import pandas as pd
import advertools as adv
from googleapiclient.discovery import build
from oauth2client.service_account import ServiceAccountCredentials
import datetime

# --- CONFIGURACI√ìN DE LA P√ÅGINA ---
st.set_page_config(page_title="SEO Index Watcher", layout="wide")

st.title("üïµÔ∏è‚Äç‚ôÇÔ∏è Monitor de Indexaci√≥n SEO")
st.markdown("Revisa qu√© URLs de tu sitemap llevan +3 d√≠as publicadas y Google ignora.")

# --- BARRA LATERAL (CONFIGURACI√ìN) ---
with st.sidebar:
    st.header("1. Configuraci√≥n")
    # Opci√≥n para subir el archivo de claves de Google de forma segura
    uploaded_key = st.file_uploader("Sube tu archivo JSON de Google Cloud", type="json")
    
    st.header("2. Objetivo")
    sitemap_url = st.text_input("URL del Sitemap", value="https://tuweb.com/sitemap.xml")
    
    days_threshold = st.slider("D√≠as de antig√ºedad para alertar", min_value=1, max_value=30, value=3)
    
    start_btn = st.button("üöÄ Iniciar Auditor√≠a")

# --- FUNCIONES ---

def get_gsc_service(key_file):
    """Autentica con la API de Google"""
    scopes = ['https://www.googleapis.com/auth/webmasters.readonly']
    # En Streamlit Cloud, esto se maneja diferente, pero para archivo local funciona as√≠:
    creds = ServiceAccountCredentials.from_json_keyfile_dict(key_file, scopes)
    service = build('searchconsole', 'v1', credentials=creds)
    return service

def inspect_url(service, site_url, page_url):
    """Consulta el estado de indexaci√≥n de una URL"""
    try:
        request = service.urlInspection().index().inspect(
            body={'inspectionUrl': page_url, 'siteUrl': site_url, 'languageCode': 'es'}
        )
        response = request.execute()
        return response['inspectionResult']['indexStatusResult']['coverageState']
    except Exception as e:
        return f"Error: {str(e)}"

# --- L√ìGICA PRINCIPAL ---

if start_btn:
    if not uploaded_key:
        st.error("‚ö†Ô∏è Por favor, sube primero el archivo JSON de credenciales.")
    elif not sitemap_url:
        st.error("‚ö†Ô∏è Introduce una URL de sitemap v√°lida.")
    else:
        import json
        key_data = json.load(uploaded_key)
        
        with st.spinner('Descargando y analizando Sitemap...'):
            try:
                # 1. Descargar Sitemap
                sitemap_df = adv.sitemap_to_df(sitemap_url)
                
                # 2. Convertir fechas y filtrar por los 'd√≠as de antig√ºedad'
                sitemap_df['lastmod'] = pd.to_datetime(sitemap_df['lastmod']).dt.tz_localize(None)
                limit_date = datetime.datetime.now() - datetime.timedelta(days=days_threshold)
                
                # Filtramos: URLs antiguas (ya deber√≠an estar indexadas)
                urls_to_check = sitemap_df[sitemap_df['lastmod'] < limit_date].copy()
                
                # Limpiamos para obtener solo las URLs (m√°ximo 50 para la demo para no agotar cuota r√°pido)
                target_urls = urls_to_check['loc'].head(50).tolist()
                
                st.info(f"üîç Se encontraron {len(target_urls)} URLs candidatas (con +{days_threshold} d√≠as). Analizando en GSC...")
                
                # 3. Conectar a API y Loop
                service = get_gsc_service(key_data)
                
                results = []
                progress_bar = st.progress(0)
                
                # Necesitamos saber la propiedad "ra√≠z" para la API (ej: https://web.com/)
                # Un truco simple es usar la base del sitemap o pedirla al usuario. 
                # Aqu√≠ intentamos extraerla de la primera URL.
                from urllib.parse import urlparse
                parsed_uri = urlparse(sitemap_url)
                site_root = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri) # sc-domain:tudominio.com si es dominio

                for i, url in enumerate(target_urls):
                    status = inspect_url(service, site_root, url)
                    
                    # Guardamos resultados si NO est√° indexada (o si quieres ver todo)
                    if status != 'INDEXED': 
                        results.append({
                            'URL': url,
                            'Publicado': urls_to_check.iloc[i]['lastmod'],
                            'Estado GSC': status
                        })
                    
                    progress_bar.progress((i + 1) / len(target_urls))

                # 4. Mostrar Resultados
                if results:
                    st.error(f"üö® Se detectaron {len(results)} URLs problem√°ticas.")
                    df_results = pd.DataFrame(results)
                    st.dataframe(df_results, use_container_width=True)
                    
                    # Bot√≥n de descarga
                    csv = df_results.to_csv(index=False).encode('utf-8')
                    st.download_button("üì• Descargar Reporte CSV", csv, "seo_audit.csv", "text/csv")
                else:
                    st.success("‚úÖ ¬°Felicidades! Todas las URLs revisadas est√°n indexadas correctamente.")

            except Exception as e:
                st.error(f"Ocurri√≥ un error: {e}")
                st.warning("Consejo: Aseg√∫rate de que el email del JSON tenga permisos en GSC y que la URL del sitemap sea correcta.")